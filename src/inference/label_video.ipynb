{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8260bbc",
   "metadata": {},
   "source": [
    "# owl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "def label_video(\n",
    "    video_path,\n",
    "    output_path,\n",
    "    text_prompt,\n",
    "    conf_threshold=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Label video with OWL-ViT zero-shot object detection.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        output_path: Path to save labeled video\n",
    "        text_prompt: Comma-separated list of objects to detect (e.g., \"bowl, cube\")\n",
    "        conf_threshold: Confidence threshold for detections\n",
    "    \"\"\"\n",
    "    # Load OWL-ViT model\n",
    "    print(\"Loading OWL-ViT model...\")\n",
    "    processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "    model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
    "    model.eval()\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Parse class names\n",
    "    class_names = [obj.strip() for obj in text_prompt.split(',')]\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing {total_frames} frames from {video_path}...\")\n",
    "    print(f\"Looking for: {text_prompt}\")\n",
    "    \n",
    "    # Colors for different classes (BGR format)\n",
    "    colors = [\n",
    "        (0, 255, 0),    # Green for first class\n",
    "        (255, 0, 0),    # Blue for second class\n",
    "        (0, 0, 255),    # Red for third class\n",
    "        (255, 255, 0),  # Cyan for fourth class\n",
    "        (255, 0, 255),  # Magenta for fifth class\n",
    "    ]\n",
    "    \n",
    "    # Statistics\n",
    "    detection_stats = {name: 0 for name in class_names}\n",
    "    \n",
    "    frame_count = 0\n",
    "    for _ in tqdm(range(total_frames)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Convert BGR to RGB for PIL\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        \n",
    "        # Process inputs\n",
    "        inputs = processor(text=class_names, images=pil_image, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get predictions\n",
    "        target_sizes = torch.Tensor([pil_image.size[::-1]]).to(device)\n",
    "        results = processor.post_process_object_detection(\n",
    "            outputs=outputs, \n",
    "            target_sizes=target_sizes, \n",
    "            threshold=conf_threshold\n",
    "        )[0]\n",
    "        \n",
    "        # Draw boxes\n",
    "        annotated_frame = frame.copy()\n",
    "        \n",
    "        for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "            x1, y1, x2, y2 = box.cpu().numpy().astype(int)\n",
    "            cls_id = label.item()\n",
    "            class_name = class_names[cls_id]\n",
    "            color = colors[cls_id % len(colors)]\n",
    "            \n",
    "            # Update statistics\n",
    "            detection_stats[class_name] += 1\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Draw label background\n",
    "            label_text = class_name\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2\n",
    "            )\n",
    "            cv2.rectangle(\n",
    "                annotated_frame,\n",
    "                (x1, y1 - text_height - baseline - 5),\n",
    "                (x1 + text_width, y1),\n",
    "                color,\n",
    "                -1\n",
    "            )\n",
    "            \n",
    "            # Draw label text\n",
    "            cv2.putText(\n",
    "                annotated_frame,\n",
    "                label_text,\n",
    "                (x1, y1 - 5),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.6,\n",
    "                (255, 255, 255),\n",
    "                2\n",
    "            )\n",
    "        \n",
    "        # Write frame\n",
    "        out.write(annotated_frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nDone! Processed {frame_count} frames.\")\n",
    "    print(f\"\\nDetection statistics:\")\n",
    "    for class_name, count in detection_stats.items():\n",
    "        print(f\"  {class_name}: {count} detections\")\n",
    "    print(f\"\\nSaved labeled video to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    VIDEO_INPUT = \"pi0.5_scene1_ep0.mp4\"\n",
    "    VIDEO_OUTPUT = \"pi0.5_scene1_ep0_labeled.mp4\"\n",
    "    \n",
    "    # Objects to detect (comma-separated)\n",
    "    PROMPT = \"bowl, cube\"\n",
    "    \n",
    "    # Confidence threshold (0.0 to 1.0)\n",
    "    CONF_THRESHOLD = 0.1\n",
    "    \n",
    "    # Get absolute paths\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    video_full_path = os.path.join(current_dir, VIDEO_INPUT)\n",
    "    output_full_path = os.path.join(current_dir, VIDEO_OUTPUT)\n",
    "    \n",
    "    # Check if video exists\n",
    "    if not os.path.exists(video_full_path):\n",
    "        print(f\"Error: Video not found at {video_full_path}\")\n",
    "        exit(1)\n",
    "    \n",
    "    label_video(\n",
    "        video_path=video_full_path,\n",
    "        output_path=output_full_path,\n",
    "        text_prompt=PROMPT,\n",
    "        conf_threshold=CONF_THRESHOLD\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb41c46",
   "metadata": {},
   "source": [
    "# yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea111b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "\n",
    "def label_video(\n",
    "    video_path,\n",
    "    output_path,\n",
    "    text_prompt,\n",
    "    conf_threshold=0.25\n",
    "):\n",
    "    \"\"\"\n",
    "    Label video with YOLO-World zero-shot object detection.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        output_path: Path to save labeled video\n",
    "        text_prompt: Comma-separated list of objects to detect (e.g., \"bowl, cube\")\n",
    "        conf_threshold: Confidence threshold for detections\n",
    "    \"\"\"\n",
    "    # Load YOLO-World model (zero-shot)\n",
    "    print(\"Loading YOLO-World model...\")\n",
    "    model = YOLO('yolov8m-world.pt')  # or 'yolov8m-world.pt', 'yolov8l-world.pt' for better accuracy\n",
    "    \n",
    "    # Set custom vocabulary (the objects you want to detect)\n",
    "    class_names = [obj.strip() for obj in text_prompt.split(',')]\n",
    "    model.set_classes(class_names)\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing {total_frames} frames from {video_path}...\")\n",
    "    print(f\"Looking for: {text_prompt}\")\n",
    "    \n",
    "    # Colors for different classes (BGR format)\n",
    "    colors = [\n",
    "        (0, 255, 0),    # Green for first class\n",
    "        (255, 0, 0),    # Blue for second class\n",
    "        (0, 0, 255),    # Red for third class\n",
    "        (255, 255, 0),  # Cyan for fourth class\n",
    "        (255, 0, 255),  # Magenta for fifth class\n",
    "    ]\n",
    "    \n",
    "    frame_count = 0\n",
    "    for _ in tqdm(range(total_frames)):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Run inference\n",
    "        results = model(frame, conf=conf_threshold, verbose=False)\n",
    "        \n",
    "        # Draw boxes manually without confidence scores\n",
    "        annotated_frame = frame.copy()\n",
    "        result = results[0]\n",
    "        \n",
    "        if result.boxes is not None:\n",
    "            boxes = result.boxes.xyxy.cpu().numpy()  # Get bounding boxes\n",
    "            classes = result.boxes.cls.cpu().numpy().astype(int)  # Get class IDs\n",
    "            \n",
    "            for i, (box, cls_id) in enumerate(zip(boxes, classes)):\n",
    "                x1, y1, x2, y2 = map(int, box)\n",
    "                class_name = class_names[cls_id]\n",
    "                color = colors[cls_id % len(colors)]\n",
    "                \n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                \n",
    "                # Draw label background\n",
    "                label = class_name\n",
    "                (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                    label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2\n",
    "                )\n",
    "                cv2.rectangle(\n",
    "                    annotated_frame,\n",
    "                    (x1, y1 - text_height - baseline - 5),\n",
    "                    (x1 + text_width, y1),\n",
    "                    color,\n",
    "                    -1\n",
    "                )\n",
    "                \n",
    "                # Draw label text\n",
    "                cv2.putText(\n",
    "                    annotated_frame,\n",
    "                    label,\n",
    "                    (x1, y1 - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.6,\n",
    "                    (255, 255, 255),\n",
    "                    2\n",
    "                )\n",
    "        \n",
    "        # Write frame\n",
    "        out.write(annotated_frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"\\nDone! Processed {frame_count} frames.\")\n",
    "    print(f\"Saved labeled video to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    VIDEO_INPUT = \"pi0.5_scene1_ep0.mp4\"\n",
    "    VIDEO_OUTPUT = \"pi0.5_scene1_ep0_labeled.mp4\"\n",
    "    \n",
    "    # Objects to detect (comma-separated)\n",
    "    PROMPT = \"rubiks cube\"\n",
    "    \n",
    "    # Confidence threshold (0.0 to 1.0)\n",
    "    CONF_THRESHOLD = 0.1\n",
    "    \n",
    "    # Get absolute paths\n",
    "    current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    video_full_path = os.path.join(current_dir, VIDEO_INPUT)\n",
    "    output_full_path = os.path.join(current_dir, VIDEO_OUTPUT)\n",
    "    \n",
    "    # Check if video exists\n",
    "    if not os.path.exists(video_full_path):\n",
    "        print(f\"Error: Video not found at {video_full_path}\")\n",
    "        exit(1)\n",
    "    \n",
    "    label_video(\n",
    "        video_path=video_full_path,\n",
    "        output_path=output_full_path,\n",
    "        text_prompt=PROMPT,\n",
    "        conf_threshold=CONF_THRESHOLD\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5eaaf2",
   "metadata": {},
   "source": [
    "# gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c5a63d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "MODEL_ID = \"gemini-robotics-er-1.5-preview\"\n",
    "\n",
    "def convert_np_to_bytes(image):\n",
    "    # convert numpy array to cv2 image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    _, image_bytes = cv2.imencode('.png', image)\n",
    "    # convert back to normal coloring\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image_bytes.tobytes()\n",
    "\n",
    "def parse_json(json_output):\n",
    "  # Parsing out the markdown fencing\n",
    "  lines = json_output.splitlines()\n",
    "  for i, line in enumerate(lines):\n",
    "    if line == \"```json\":\n",
    "      # Remove everything before \"```json\"\n",
    "      json_output = \"\\n\".join(lines[i + 1 :])\n",
    "      # Remove everything after the closing \"```\"\n",
    "      json_output = json_output.split(\"```\")[0]\n",
    "      break  # Exit the loop once \"```json\" is found\n",
    "  return json_output\n",
    "\n",
    "def query_gemini(image_bytes):\n",
    "    # PROMPT = \"\"\"\n",
    "    #         Return bounding boxes as a JSON array with labels. Never return masks orcode fencing.\n",
    "    #         Find all the objects on the table.\n",
    "    #         The label returned should be an identifying name for the object detected.\n",
    "    #         If an object is present multiple times, name each according to their UNIQUE CHARACTERISTIC\n",
    "    #         (colors, size, position, etc.)\n",
    "    #         The format should be as follows:\n",
    "    #         [{\"box_2d\": [ymin, xmin, ymax, xmax], \"label\": <label for the object>}]\n",
    "    #         normalized to 0-1000. The values in box_2d must only be integers.\n",
    "    #         \"\"\"\n",
    "\n",
    "    PROMPT = \"\"\"\n",
    "            Point to all the objects on the table. The label returned\n",
    "            should be an identifying name for the object detected. Don't label the color of the object.\n",
    "            The answer should follow the json format: [{\"point\": <point>,\n",
    "            \"label\": <label1>}, ...]. The points are in [y, x] format\n",
    "            normalized to 0-1000.\n",
    "    \"\"\"\n",
    "\n",
    "    image_response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=[\n",
    "            types.Part.from_bytes(\n",
    "                data=image_bytes,\n",
    "                mime_type='image/png',\n",
    "            ),\n",
    "            PROMPT\n",
    "        ],\n",
    "        config = types.GenerateContentConfig(\n",
    "            temperature=0.5,\n",
    "            thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return json.loads(parse_json(image_response.text))\n",
    "\n",
    "def scale_bounding_boxes(json_output, image):\n",
    "    y_scale = image.shape[0] / 1000\n",
    "    x_scale = image.shape[1] / 1000\n",
    "    scaled_json_output = []\n",
    "    for item in json_output:\n",
    "        scaled_item = {\n",
    "            'box_2d': [int(item['box_2d'][0] * y_scale), int(item['box_2d'][1] * x_scale), int(item['box_2d'][2] * y_scale), int(item['box_2d'][3] * x_scale)],\n",
    "            'label': item['label']\n",
    "        }\n",
    "        scaled_json_output.append(scaled_item)\n",
    "    return scaled_json_output\n",
    "\n",
    "def scale_points(json_output, image):\n",
    "    y_scale = image.shape[0] / 1000\n",
    "    x_scale = image.shape[1] / 1000\n",
    "    scaled_json_output = []\n",
    "    for item in json_output:\n",
    "        scaled_item = {\n",
    "            'point': [int(item['point'][0] * y_scale), int(item['point'][1] * x_scale)],\n",
    "            'label': item['label'].lower()  # Force lowercase\n",
    "        }\n",
    "        scaled_json_output.append(scaled_item)\n",
    "    return scaled_json_output\n",
    "\n",
    "def plot_points(image, json_output):\n",
    "    annotated_image = image.copy()\n",
    "    for item in json_output:\n",
    "        y, x = item['point']\n",
    "        label = item['label']\n",
    "        cv2.circle(annotated_image, (x, y), 5, (0, 255, 255), -1)\n",
    "        text_width, text_height = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)[0]\n",
    "        cv2.rectangle(annotated_image, (x + 8, y - text_height), (x + text_width, y), (0, 0, 0), -1)\n",
    "        cv2.putText(annotated_image, label, (x + 8, y ), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    return annotated_image\n",
    "\n",
    "def plot_bounding_boxes(image, json_output):\n",
    "    annotated_image = image.copy()\n",
    "\n",
    "    colors = [\n",
    "        (0, 0, 255),      # red\n",
    "        (0, 255, 0),      # green\n",
    "        (255, 0, 0),      # blue\n",
    "        (0, 255, 255),    # yellow\n",
    "        (0, 165, 255),    # orange\n",
    "        (255, 192, 203),  # pink\n",
    "        (128, 0, 128),    # purple\n",
    "        (42, 42, 165),    # brown\n",
    "        (128, 128, 128),  # gray\n",
    "        (255, 255, 0),    # cyan\n",
    "    ]\n",
    "\n",
    "    for i, item in enumerate(json_output):\n",
    "        color = colors[i % len(colors)]\n",
    "        y1, x1, y2, x2 = item['box_2d']\n",
    "        label = item['label']\n",
    "\n",
    "        cv2.rectangle(annotated_image, (x1, y1), (x2, y2), color, 2) # draw rectangle\n",
    "        # draw label with background\n",
    "        text_width, text_height = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.75, 2)[0]\n",
    "        cv2.rectangle(annotated_image, (x1, y2 - text_height - 10), (x1 + text_width, y2), color, -1)\n",
    "        cv2.putText(annotated_image, label, (x1, y2 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 0), 2)\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "185db0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 151 frames...\n",
      "Frame 0: ['bowl', \"rubik's cube\"]\n",
      "Frame 10: ['bowl', \"rubik's cube\"]\n",
      "Frame 20: [\"rubik's cube\", 'bowl']\n",
      "Processed 30/151 frames\n",
      "Frame 30: [\"rubik's cube\", 'bowl']\n",
      "Frame 40: [\"rubik's cube\", 'bowl']\n",
      "Frame 50: [\"rubik's cube\", 'bowl']\n",
      "Processed 60/151 frames\n",
      "Frame 60: ['bowl', \"rubik's cube\"]\n",
      "Frame 70: [\"rubik's cube\", 'bowl']\n",
      "Frame 80: ['bowl', \"rubik's cube\"]\n",
      "Processed 90/151 frames\n",
      "Frame 90: ['bowl', \"rubik's cube\"]\n",
      "Frame 100: ['bowl', \"rubik's cube\"]\n",
      "Frame 110: ['bowl', \"rubik's cube\"]\n",
      "Processed 120/151 frames\n",
      "Frame 120: ['bowl', \"rubik's cube\"]\n",
      "Frame 130: [\"rubik's cube\", 'blue bowl']\n",
      "Frame 140: [\"rubik's cube\"]\n",
      "Processed 150/151 frames\n",
      "Frame 150: [\"rubik's cube in a bowl\"]\n",
      "Done! Saved to pi0.5_scene1_ep0_gemini_labeled.mp4\n"
     ]
    }
   ],
   "source": [
    "def label_video_gemini(\n",
    "    video_path,\n",
    "    output_path,\n",
    "    sample_every_n_frames=1  # Process every Nth frame (1 = all frames)\n",
    "):\n",
    "    \"\"\"\n",
    "    Label video using Gemini object detection.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(f\"Processing {total_frames} frames...\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    last_detections = None  # Cache detections between sampled frames\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Only query Gemini every N frames (API rate limits + cost)\n",
    "        if frame_idx % sample_every_n_frames == 0:\n",
    "            try:\n",
    "                image_bytes = convert_np_to_bytes(frame)\n",
    "                json_output = query_gemini(image_bytes)\n",
    "                last_detections = scale_points(json_output, frame)\n",
    "                print(f\"Frame {frame_idx}: {[d['label'] for d in last_detections]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Frame {frame_idx}: Error - {e}\")\n",
    "        \n",
    "        # Draw detections on frame\n",
    "        if last_detections:\n",
    "            annotated_frame = plot_points(frame, last_detections)\n",
    "        else:\n",
    "            annotated_frame = frame\n",
    "        \n",
    "        out.write(annotated_frame)\n",
    "        frame_idx += 1\n",
    "        \n",
    "        if frame_idx % 30 == 0:\n",
    "            print(f\"Processed {frame_idx}/{total_frames} frames\")\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Done! Saved to {output_path}\")\n",
    "\n",
    "# Run it\n",
    "label_video_gemini(\n",
    "    video_path=\"pi0.5_scene1_ep0.mp4\",\n",
    "    output_path=\"pi0.5_scene1_ep0_gemini_labeled.mp4\",\n",
    "    sample_every_n_frames=10  # Query Gemini every 10 frames to save API calls\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "072fa0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0\n",
      "Frame 10\n",
      "Frame 20\n",
      "Frame 30\n",
      "Frame 40\n",
      "Frame 50\n",
      "Frame 60\n",
      "Frame 70\n",
      "Frame 80\n",
      "Frame 90\n",
      "Frame 100\n",
      "Frame 110\n",
      "Frame 120\n",
      "Frame 130\n",
      "Frame 140\n",
      "Frame 150\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def label_combined_video(video_path, output_path, sample_every_n_frames=10):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    half_width = width // 2\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    frame_idx = 0\n",
    "    last_external_detections = None\n",
    "    last_wrist_detections = None\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        external_frame = frame[:, :half_width]\n",
    "        wrist_frame = frame[:, half_width:]\n",
    "        \n",
    "        if frame_idx % sample_every_n_frames == 0:\n",
    "            print(f\"Frame {frame_idx}\")\n",
    "            try:\n",
    "                # Label external (left) side\n",
    "                ext_bytes = convert_np_to_bytes(external_frame)\n",
    "                ext_json = query_gemini(ext_bytes)\n",
    "                last_external_detections = scale_points(ext_json, external_frame)\n",
    "                \n",
    "                # Label wrist (right) side\n",
    "                wrist_bytes = convert_np_to_bytes(wrist_frame)\n",
    "                wrist_json = query_gemini(wrist_bytes)\n",
    "                last_wrist_detections = scale_points(wrist_json, wrist_frame)\n",
    "            except Exception as e:\n",
    "                print(f\"Frame {frame_idx}: Error - {e}\")\n",
    "        \n",
    "        # Draw on each half\n",
    "        if last_external_detections:\n",
    "            external_frame = plot_points(external_frame, last_external_detections)\n",
    "        if last_wrist_detections:\n",
    "            wrist_frame = plot_points(wrist_frame, last_wrist_detections)\n",
    "        \n",
    "        # Combine back together\n",
    "        combined = np.hstack([external_frame, wrist_frame])\n",
    "        out.write(combined)\n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "# Use it\n",
    "label_combined_video(\"pi0.5_scene1_ep0.mp4\", \"pi0.5_scene1_ep0_both_labeled.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
